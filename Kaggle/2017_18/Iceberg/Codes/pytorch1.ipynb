{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_json('train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['band_1'] = data['band_1'].apply(lambda x: np.array(x).reshape(75,75))\n",
    "data['band_2'] = data['band_2'].apply(lambda x: np.array(x).reshape(75,75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = data.sample(frac=0.8)\n",
    "val = data[~data.isin(train)].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "band_1_tr = np.concatenate([im for im in train['band_1']]).reshape(-1, 75, 75)\n",
    "band_2_tr = np.concatenate([im for im in train['band_2']]).reshape(-1, 75, 75)\n",
    "full_img_tr = np.stack([band_1_tr, band_2_tr], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1283, 2, 75, 75)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_img_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "band_1_val = np.concatenate([im for im in val['band_1']]).reshape(-1, 75, 75)\n",
    "band_2_val = np.concatenate([im for im in val['band_2']]).reshape(-1, 75, 75)\n",
    "full_img_val = np.stack([band_1_val, band_2_val], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam, Adadelta\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = train['is_iceberg'].values\n",
    "y_train = np.append(y_train.reshape(-1,1), np.zeros([len(y_train),1]),1)\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i,0] == 0:\n",
    "        y_train[i,1] = 1\n",
    "#y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val = val['is_iceberg'].values\n",
    "y_val = np.append(y_val.reshape(-1,1), np.zeros([len(y_val),1]),1)\n",
    "for i in range(len(y_val)):\n",
    "    if y_val[i,0] == 0:\n",
    "        y_val[i,1] = 1\n",
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_imgs = torch.from_numpy(full_img_tr).float()\n",
    "train_targets = torch.from_numpy(y_train).long()\n",
    "train_dataset = TensorDataset(train_imgs, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 75, 75])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_imgs = torch.from_numpy(full_img_val).float()\n",
    "val_targets = torch.from_numpy(y_val).long()\n",
    "val_dataset = TensorDataset(val_imgs, val_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.batch = nn.BatchNorm2d(2)\n",
    "        self.conv1 = nn.Conv2d(2, 12, kernel_size=5, padding=1)\n",
    "        init.xavier_normal(self.conv1.weight, gain = np.sqrt(2.0))\n",
    "        init.constant(self.conv1.bias, 0.1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(12, 20, kernel_size=5, padding=1)\n",
    "        init.xavier_normal(self.conv2.weight, gain = np.sqrt(2.0))\n",
    "        init.constant(self.conv2.bias, 0.1)\n",
    "        self.fc1 = nn.Linear(20 * 17 * 17, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "    def forward(self, x):\n",
    "        x = self.batch(x)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.Softmax(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 6, kernel_size=5, padding=0)\n",
    "        init.xavier_normal(self.conv1.weight, gain = np.sqrt(2.0))\n",
    "        init.constant(self.conv1.bias, 0.1)\n",
    "        self.conv2 = nn.Conv2d(6, 12, kernel_size=5, padding=0, stride = 2)\n",
    "        init.xavier_normal(self.conv2.weight, gain = np.sqrt(2.0))\n",
    "        init.constant(self.conv2.bias, 0.1)\n",
    "        self.conv3 = nn.Conv2d(12, 24, kernel_size=5, padding=0, stride = 2)\n",
    "        init.xavier_normal(self.conv3.weight, gain = np.sqrt(2.0))\n",
    "        init.constant(self.conv3.bias, 0.1)\n",
    "        self.fc1 = nn.Linear(24 * 15 * 15, 120)\n",
    "        self.fc2 = nn.Linear(120, 2)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(2, 6, kernel_size=5, padding=0),\n",
    "            nn.ReLU())\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 12, kernel_size=5, padding=0, stride = 2),\n",
    "            nn.ReLU())\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(12, 24, kernel_size=5, padding=0, stride = 2),\n",
    "            nn.ReLU())\n",
    "        self.fc1 = nn.Sequential(\n",
    "                nn.Linear(15*15*24, 200),\n",
    "                nn.ReLU())\n",
    "        self.fc2 = nn.Linear(200,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(95256/24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net (\n",
       "  (conv1): Conv2d(2, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (conv3): Conv2d(12, 24, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (fc1): Linear (5400 -> 120)\n",
       "  (fc2): Linear (120 -> 2)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adadelta(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "######     Computes and stores the average and current value\n",
    "    def __init__(self, window_size=None):\n",
    "        self.length = 0\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.window_size = window_size\n",
    "    def reset(self):\n",
    "        self.length = 0\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "    def update(self, val, n=1):\n",
    "        if self.window_size and (self.count >= self.window_size):\n",
    "            self.reset()\n",
    "            self.val = val\n",
    "            self.sum += val * n\n",
    "            self.count += n\n",
    "            self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    y_true = y_true.float()\n",
    "    _, y_pred = torch.max(y_pred, dim=-1)\n",
    "    return (y_pred.float() == y_true).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "def fit(train, val, epochs, batch_size):\n",
    "    print('train on {} images validate on {} images'.format(len(train), len(val)))\n",
    "    model.train()\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val, batch_size=batch_size, shuffle=False)\n",
    "    for epoch in tqdm_notebook(range(epochs), total=epochs):\n",
    "        running_loss = AverageMeter()\n",
    "        running_accuracy = AverageMeter()\n",
    "        val_loss_meter = AverageMeter()\n",
    "        val_acc_meter = AverageMeter()\n",
    "        pbar = tqdm_notebook(train_loader, total=len(train_loader))\n",
    "        for data, target in pbar:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            acc = accuracy(target.data, output.data)\n",
    "            running_loss.update(loss.data[0])\n",
    "            running_accuracy.update(acc)\n",
    "            pbar.set_description(\"[ loss: {:.4f} | acc: {:.4f} ] \".format(running_loss.avg, running_accuracy.avg))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(\"[ loss: {:.4f} | acc: {:.4f} ] \".format(running_loss.avg, running_accuracy.avg))\n",
    "        for val_data, val_target in val_loader:\n",
    "            val_data, val_target = Variable(val_data), Variable(val_target)\n",
    "            output = model(val_data)\n",
    "            val_loss = criterion(output, val_target)\n",
    "            val_acc = accuracy(val_target.data, output.data)\n",
    "            val_loss_meter.update(val_loss.data[0])\n",
    "            val_acc_meter.update(val_acc)\n",
    "        pbar.set_description(\"[ loss: {:.4f} | acc: {:.4f} | vloss: {:.4f} | vacc: {:.4f} ] \".format(running_loss.avg, running_accuracy.avg, val_loss_meter.avg, val_acc_meter.avg))\n",
    "        print(\"[ loss: {:.4f} | acc: {:.4f} | vloss: {:.4f} | vacc: {:.4f} ] \".format(running_loss.avg, running_accuracy.avg, val_loss_meter.avg, val_acc_meter.avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 1283 images validate on 321 images\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e6ed395e84498ba5bfd534d2bb8075"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb777f7ce804fa8a39eca998b1fd264"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "multi-target not supported at /opt/conda/conda-bld/pytorch_1501969512886/work/pytorch-0.1.12/torch/lib/THNN/generic/ClassNLLCriterion.c:20",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-a7ddb03a8595>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-611e76452c61>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(train, val, epochs, batch_size)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mrunning_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda3/envs/virtual_platform/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda3/envs/virtual_platform/lib/python3.5/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         return F.cross_entropy(input, target,\n\u001b[0;32m--> 321\u001b[0;31m                                self.weight, self.size_average)\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda3/envs/virtual_platform/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average)\u001b[0m\n\u001b[1;32m    531\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     \"\"\"\n\u001b[0;32m--> 533\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda3/envs/virtual_platform/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Expected 2 or 4 dimensions (got {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda3/envs/virtual_platform/lib/python3.5/site-packages/torch/nn/_functions/thnn/auto.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         getattr(self._backend, update_output.name)(self._backend.library_state, input, target,\n\u001b[0;32m---> 41\u001b[0;31m                                                    output, *self.additional_args)\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: multi-target not supported at /opt/conda/conda-bld/pytorch_1501969512886/work/pytorch-0.1.12/torch/lib/THNN/generic/ClassNLLCriterion.c:20"
     ]
    }
   ],
   "source": [
    "fit(train_dataset, val_dataset, 3, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " (0 ,0 ,.,.) = \n",
       "  -1.4889e-01  4.5710e-02 -5.0143e-02 -2.7000e-03 -6.6145e-04\n",
       "   7.8826e-02 -1.2682e-01 -3.5409e-02  5.9538e-02 -1.0565e-01\n",
       "  -1.3036e-01  1.2130e-02 -7.2381e-02 -7.7606e-02  4.9357e-03\n",
       "   4.4284e-02  3.7420e-02  5.4858e-02 -4.7745e-02  3.3850e-02\n",
       "  -1.6332e-02 -4.7159e-03 -5.0119e-02 -5.2910e-02  3.1945e-02\n",
       " \n",
       " (0 ,1 ,.,.) = \n",
       "  -9.8006e-02  2.1212e-02 -6.9720e-03  1.0578e-01 -7.4700e-03\n",
       "   3.5693e-02  1.2701e-01 -4.9049e-02 -1.2123e-02 -9.4965e-02\n",
       "   5.7547e-02  4.2047e-03 -1.1264e-01  4.8172e-02  3.4292e-02\n",
       "  -6.5764e-02  2.4009e-02  8.4801e-02  5.0087e-02  1.4401e-02\n",
       "   5.4766e-02  5.3015e-02 -9.1106e-02  1.1502e-02 -4.2703e-02\n",
       " \n",
       " (0 ,2 ,.,.) = \n",
       "   2.9371e-02 -3.8912e-02 -6.3785e-02 -3.7863e-02 -1.0076e-01\n",
       "  -5.1366e-02 -7.2622e-04 -5.0236e-02  4.2120e-02  3.8804e-02\n",
       "  -6.6644e-02 -1.6767e-02 -3.2754e-02  8.4586e-02  4.6213e-02\n",
       "  -9.8049e-02  8.7449e-04 -8.4819e-02  1.1458e-01 -4.3753e-02\n",
       "   4.5621e-02  4.2223e-02  1.8311e-01  5.9490e-03 -1.4102e-01\n",
       "    ...\n",
       " \n",
       " (0 ,9 ,.,.) = \n",
       "   5.6687e-02  2.8008e-02 -1.0770e-01  8.7788e-02 -1.3453e-02\n",
       "   3.3484e-02 -4.5873e-02  1.3265e-01  6.7756e-02 -6.6157e-02\n",
       "   1.1650e-01  2.1010e-02 -6.3595e-02  3.3477e-03  4.6165e-02\n",
       "  -6.5474e-03  4.5236e-02  1.5971e-02  6.8829e-02  6.1691e-02\n",
       "  -5.0828e-02  1.0041e-01 -1.0446e-01 -5.8314e-02  1.1527e-01\n",
       " \n",
       " (0 ,10,.,.) = \n",
       "  -3.2381e-02 -1.4947e-01  2.4036e-03  1.7437e-01 -6.5007e-02\n",
       "   1.4675e-02 -4.4353e-02 -1.0842e-01 -5.2338e-02  9.5358e-02\n",
       "  -4.2649e-02  9.5825e-02  4.8359e-02 -4.1982e-02  4.4188e-02\n",
       "   5.3107e-02 -1.9430e-02  8.0173e-02  2.5661e-03 -5.2542e-02\n",
       "   3.8397e-02  5.4548e-02 -2.6073e-02  4.6926e-02  1.4129e-01\n",
       " \n",
       " (0 ,11,.,.) = \n",
       "  -2.0935e-02 -5.9271e-02  5.9944e-02 -9.8448e-02  1.0846e-01\n",
       "   1.0137e-01 -9.7565e-02 -9.4693e-02 -9.5978e-02 -9.5662e-02\n",
       "   1.4181e-02 -1.6163e-02 -9.1311e-03 -1.7425e-02 -1.0454e-01\n",
       "  -6.9010e-03  2.0101e-02  5.3402e-02  1.0326e-02 -2.6219e-02\n",
       "   8.4456e-02 -3.2230e-02  2.4930e-02  1.1979e-01 -1.0930e-01\n",
       "      ⋮ \n",
       " \n",
       " (1 ,0 ,.,.) = \n",
       "   7.6543e-02  3.5564e-04  1.0483e-01  3.5126e-02  2.5430e-02\n",
       "   1.0923e-02 -1.2236e-01 -3.3055e-02 -3.3077e-02 -1.6679e-03\n",
       "  -9.7575e-02 -2.8351e-02  4.6887e-02 -1.7949e-02  1.8763e-02\n",
       "   2.0778e-02 -3.0389e-02 -1.1068e-02  3.8102e-02  5.4717e-02\n",
       "  -1.0618e-02  6.5045e-02  1.0278e-01 -4.9033e-02 -5.3722e-02\n",
       " \n",
       " (1 ,1 ,.,.) = \n",
       "  -1.0076e-01  5.1381e-02 -5.8200e-02 -1.3174e-01 -5.8944e-02\n",
       "   2.6477e-03  6.3259e-03  1.4697e-01 -2.0935e-02 -1.8993e-02\n",
       "   3.0581e-03  5.5511e-04 -6.3099e-02  8.3195e-02 -8.8017e-02\n",
       "   3.5472e-02  3.9392e-03  5.7632e-03  1.4828e-01 -3.1810e-02\n",
       "   1.1346e-01  2.2488e-02 -1.0692e-02 -4.2882e-02 -7.3670e-02\n",
       " \n",
       " (1 ,2 ,.,.) = \n",
       "   6.6294e-02 -1.4791e-02  3.8547e-02 -2.3228e-02 -4.4522e-03\n",
       "   9.0625e-02 -1.4857e-02  3.5396e-03 -3.4047e-02  6.7296e-03\n",
       "   7.4806e-02  6.5878e-02  1.6906e-02 -7.9654e-03  2.4850e-02\n",
       "  -7.6227e-02 -4.1939e-02  6.6420e-02 -2.7974e-02 -3.0219e-02\n",
       "   4.2764e-02 -2.0318e-02 -2.1453e-01  1.0922e-02 -1.1582e-01\n",
       "    ...\n",
       " \n",
       " (1 ,9 ,.,.) = \n",
       "   2.2280e-02  6.3072e-02  2.4323e-02  1.2027e-02  1.3024e-02\n",
       "   4.8958e-02 -2.5721e-02 -8.9174e-02  9.3020e-02 -1.0545e-01\n",
       "   3.0385e-02 -4.4786e-02 -1.2526e-02  4.7343e-02 -3.5691e-02\n",
       "  -1.0451e-02  9.3167e-02 -3.3348e-02  1.0758e-01  1.0326e-02\n",
       "  -2.3989e-03 -9.2974e-03 -6.7411e-02 -3.5429e-02  5.5648e-02\n",
       " \n",
       " (1 ,10,.,.) = \n",
       "   2.2118e-02 -4.5676e-02  9.8461e-03  1.2587e-02 -8.8648e-03\n",
       "   6.2354e-02 -6.5879e-02  2.0410e-01 -1.9723e-01  1.8514e-03\n",
       "  -2.0332e-02 -7.3645e-02 -9.1114e-02  6.2141e-02  1.4992e-02\n",
       "  -5.7777e-02 -1.9362e-02 -1.1181e-01 -1.1327e-02 -4.9578e-02\n",
       "  -4.0335e-02 -1.4725e-02  1.6909e-02  3.3362e-02  6.7928e-02\n",
       " \n",
       " (1 ,11,.,.) = \n",
       "   7.0097e-02 -4.6512e-02 -3.0830e-02 -5.5790e-03 -6.2005e-02\n",
       "   9.5184e-02  9.8706e-02 -1.6988e-02  2.9453e-02  4.0008e-02\n",
       "   1.7508e-01  1.7632e-02  4.8573e-03  8.4322e-03  4.8269e-03\n",
       "  -2.0349e-02 -8.7034e-03  5.0881e-02 -1.0440e-01 -1.7401e-02\n",
       "   1.3928e-01 -5.0567e-02 -1.3605e-01  3.6393e-02  2.2050e-02\n",
       "      ⋮ \n",
       " \n",
       " (2 ,0 ,.,.) = \n",
       "  -9.1418e-02 -7.6448e-02  7.8282e-03  5.9320e-02 -2.4184e-02\n",
       "   1.6581e-01 -5.5342e-03 -5.7732e-02 -8.2367e-02 -4.0236e-02\n",
       "   6.0244e-02  8.9431e-02 -1.6621e-02 -1.8885e-02  1.1587e-02\n",
       "   7.4912e-02 -1.0516e-01 -1.0736e-01 -1.7717e-02  5.0960e-02\n",
       "   6.0701e-02 -6.7256e-02  6.1492e-02 -5.9440e-02  1.3314e-01\n",
       " \n",
       " (2 ,1 ,.,.) = \n",
       "   3.2573e-03  7.7511e-02 -3.0874e-02 -7.3224e-02  3.8300e-03\n",
       "  -4.8050e-02  2.5014e-02 -2.8461e-02  9.7838e-02 -3.6677e-02\n",
       "   5.1963e-03  3.2457e-02  9.3437e-02  2.0678e-02  3.5223e-02\n",
       "   4.0563e-03  4.6662e-02 -6.3388e-02  6.2188e-02 -7.6629e-02\n",
       "   2.9058e-02  1.0634e-02  1.0267e-01  1.2985e-02  1.0573e-01\n",
       " \n",
       " (2 ,2 ,.,.) = \n",
       "  -2.6457e-02 -8.3025e-02  7.0073e-02 -3.3685e-02  7.5449e-02\n",
       "   2.4234e-02 -1.8863e-03  4.9403e-03  1.0963e-03 -9.4713e-02\n",
       "  -6.0885e-02  1.0730e-03 -2.6197e-02  1.7475e-01 -7.3200e-02\n",
       "   1.5023e-02 -8.0328e-02  3.6640e-02 -3.9099e-02  1.3880e-01\n",
       "   1.0328e-01 -8.4312e-02  4.2677e-02  5.2500e-02 -6.8806e-03\n",
       "    ...\n",
       " \n",
       " (2 ,9 ,.,.) = \n",
       "   5.9340e-03 -2.6188e-02 -2.3099e-02 -1.6456e-02 -1.2203e-01\n",
       "   2.7401e-02 -2.4797e-02  4.7846e-03 -3.3828e-02  1.2145e-01\n",
       "   5.1297e-02  4.1768e-02 -6.6404e-02  4.1266e-02 -4.9408e-03\n",
       "  -4.2771e-02 -9.6703e-02  1.7446e-04  1.9334e-02  1.5249e-01\n",
       "   8.1242e-03 -1.0516e-01  1.3496e-01 -7.8177e-02 -1.5296e-01\n",
       " \n",
       " (2 ,10,.,.) = \n",
       "  -1.2783e-01  9.2650e-02 -3.9346e-02 -1.5550e-01 -2.1954e-02\n",
       "   1.0628e-02  7.1071e-02 -1.3852e-01  1.3222e-02 -1.3831e-02\n",
       "  -8.4790e-02  1.4148e-02 -6.8157e-02 -3.8662e-02  2.7015e-02\n",
       "   3.1804e-02 -5.4051e-02 -7.6220e-02 -1.2337e-03  1.2107e-02\n",
       "  -3.2675e-02 -9.1742e-02 -5.5663e-03  1.7693e-01 -3.2213e-02\n",
       " \n",
       " (2 ,11,.,.) = \n",
       "  -1.7905e-02  1.9839e-02  5.9989e-02 -2.2015e-02  5.6364e-03\n",
       "  -1.3923e-01 -4.2201e-02 -4.4756e-02  5.6923e-02  9.0493e-02\n",
       "  -4.2923e-02 -9.4619e-02  5.9636e-02 -2.2090e-02  1.0847e-01\n",
       "   1.2006e-02  1.1960e-01 -7.4534e-02 -2.8728e-03  9.4241e-02\n",
       "   3.3885e-02 -5.9832e-02  6.3665e-02 -8.3219e-03  8.8541e-03\n",
       " ...   \n",
       "      ⋮ \n",
       " \n",
       " (17,0 ,.,.) = \n",
       "  -2.4337e-02 -3.7794e-02  8.2316e-02  1.7717e-02  3.7023e-02\n",
       "   2.9161e-04  1.0751e-01  7.4536e-02  7.1610e-02 -8.2689e-02\n",
       "  -2.6566e-02 -2.5759e-02  5.1337e-02 -2.5536e-02 -5.2582e-03\n",
       "   4.7921e-02 -1.7106e-01  4.5861e-02 -1.5673e-01  3.9164e-02\n",
       "   1.0186e-01  5.1133e-02 -9.9469e-02 -5.8218e-02 -4.2821e-03\n",
       " \n",
       " (17,1 ,.,.) = \n",
       "   8.7160e-02  6.0243e-02  1.4619e-02 -1.4547e-04  4.5011e-02\n",
       "  -1.3642e-01 -5.4974e-02  3.9586e-02  5.9869e-02  3.4420e-02\n",
       "  -3.4741e-02 -2.4758e-02  4.5208e-02  4.6750e-02 -4.9942e-02\n",
       "  -2.6829e-02  4.8759e-03  1.5894e-02 -6.0547e-02 -4.5598e-02\n",
       "  -1.1838e-01  1.0153e-01 -2.2275e-02 -8.7879e-02 -1.1082e-01\n",
       " \n",
       " (17,2 ,.,.) = \n",
       "   1.5140e-02  1.8222e-02 -9.4681e-02 -1.2694e-02 -6.3261e-03\n",
       "  -2.7186e-02  3.8415e-02 -2.8787e-02  9.2654e-03 -5.7984e-02\n",
       "  -3.4078e-02 -1.5487e-02 -5.7393e-02  9.6770e-02  3.8895e-02\n",
       "   9.7579e-02 -3.3763e-02  8.2183e-02 -7.4657e-02  8.6716e-02\n",
       "  -2.5910e-02 -4.5639e-03 -6.6595e-02 -7.4939e-02 -1.7769e-02\n",
       "    ...\n",
       " \n",
       " (17,9 ,.,.) = \n",
       "   2.2566e-02 -9.1839e-02 -2.6364e-02  5.0872e-03  8.3339e-02\n",
       "   1.0181e-02 -1.0390e-01  1.4736e-02 -5.4447e-02 -1.5535e-02\n",
       "   7.2478e-03 -4.0379e-02 -1.0521e-01 -2.1598e-02 -2.4701e-04\n",
       "   2.0128e-02 -1.3176e-02  1.0662e-01  1.0402e-01  4.8921e-02\n",
       "  -2.8795e-02 -2.7941e-02  2.4560e-02 -1.0783e-01 -1.4935e-02\n",
       " \n",
       " (17,10,.,.) = \n",
       "   8.5985e-02 -9.7980e-02  1.5294e-03 -3.8933e-02 -6.5218e-03\n",
       "   4.5333e-02  2.8226e-02  3.4734e-02  1.1828e-01 -1.2109e-01\n",
       "  -8.0705e-03 -7.8046e-02 -4.4282e-02  3.9975e-02 -4.3904e-02\n",
       "   1.1069e-01 -8.3823e-03 -1.7024e-01 -2.5561e-02  9.7012e-02\n",
       "   1.0648e-01 -5.2481e-02  6.2954e-02 -8.1119e-02  3.7505e-02\n",
       " \n",
       " (17,11,.,.) = \n",
       "  -8.2738e-02  1.3723e-01 -3.3800e-02 -6.1219e-02 -1.8993e-02\n",
       "  -1.4173e-03  1.1172e-01  5.8842e-02 -4.1355e-02  1.6374e-02\n",
       "   3.3288e-02  4.4406e-02  2.2663e-02 -2.0437e-02  2.6852e-02\n",
       "   1.1810e-01 -9.4317e-02  6.1392e-02  4.4368e-02 -4.1933e-02\n",
       "   4.0350e-02  1.9004e-01  7.2770e-02 -9.7674e-02 -6.1505e-02\n",
       "      ⋮ \n",
       " \n",
       " (18,0 ,.,.) = \n",
       "  -5.7735e-02  6.0011e-02  1.9678e-02 -1.1417e-01  9.4215e-02\n",
       "   7.7404e-03 -5.8384e-02 -1.2799e-02 -1.4474e-01  3.0921e-02\n",
       "   7.5525e-02  1.0439e-02 -2.3659e-02 -7.8734e-03  2.7635e-02\n",
       "  -8.2252e-02  5.0334e-02  7.5589e-02  2.8378e-02 -3.4081e-02\n",
       "   2.2833e-02 -3.3451e-02 -1.0711e-01  8.0634e-02  1.9381e-02\n",
       " \n",
       " (18,1 ,.,.) = \n",
       "   9.4944e-02  5.3935e-02  2.0458e-02 -7.0541e-02  4.1234e-02\n",
       "  -3.1714e-02 -3.5752e-02 -2.8610e-02  1.4223e-01 -5.2689e-02\n",
       "  -6.7491e-02 -3.4108e-02 -7.3757e-02 -1.2666e-02 -1.2966e-01\n",
       "  -5.8063e-02 -2.7735e-02 -5.1568e-02 -3.4880e-02  4.7134e-02\n",
       "   2.5509e-02 -9.7461e-02 -1.2911e-02  9.1900e-02 -3.7514e-02\n",
       " \n",
       " (18,2 ,.,.) = \n",
       "   5.6249e-02  6.1752e-02 -5.0005e-02 -9.6306e-02  1.5410e-02\n",
       "  -1.4489e-01 -2.7995e-02 -5.7784e-02 -9.4430e-02  3.6289e-02\n",
       "   3.8241e-02  3.7040e-02 -9.7673e-02 -3.1642e-02 -1.4504e-02\n",
       "  -4.4873e-02 -5.0176e-04  1.1168e-01  1.7439e-01 -1.1967e-01\n",
       "  -3.7669e-02  5.0599e-02  5.1913e-02  5.5988e-02 -1.9755e-02\n",
       "    ...\n",
       " \n",
       " (18,9 ,.,.) = \n",
       "   2.4804e-02 -2.0184e-02 -1.2615e-02 -9.7870e-03 -4.4671e-03\n",
       "   1.6376e-02 -3.7398e-02  4.4938e-03 -4.2041e-02  6.2699e-02\n",
       "   3.3330e-02 -1.0152e-01  5.3955e-02  5.3487e-02 -6.0873e-02\n",
       "  -2.5136e-01  5.8351e-02 -2.2644e-02 -8.4744e-03 -9.2439e-02\n",
       "  -2.0002e-02 -2.0193e-02 -1.5815e-02 -5.3317e-02 -5.3515e-02\n",
       " \n",
       " (18,10,.,.) = \n",
       "   1.4851e-01  3.3883e-02  6.7953e-04 -6.3072e-02 -1.2413e-01\n",
       "  -3.2378e-02  2.3063e-02  3.4965e-02  1.3722e-01  8.9748e-02\n",
       "   3.8853e-02  1.0513e-02 -1.0379e-01  8.4840e-02 -8.3764e-02\n",
       "  -3.8930e-02 -6.8019e-02 -6.2003e-02  5.2532e-02  4.7445e-02\n",
       "  -1.1776e-02 -1.7065e-02  5.3698e-02  6.2383e-02 -1.9359e-02\n",
       " \n",
       " (18,11,.,.) = \n",
       "   7.0766e-02 -2.5976e-02 -3.0592e-02 -4.2850e-02 -1.7541e-02\n",
       "  -9.9052e-02 -1.9984e-02 -8.3368e-02 -1.6985e-01  3.7931e-02\n",
       "  -1.2905e-01  8.2591e-03  6.2499e-02 -1.4508e-02 -1.6656e-02\n",
       "  -6.7753e-02  8.1469e-02  5.2482e-02 -5.7300e-02 -1.0147e-01\n",
       "  -6.0436e-02 -9.5461e-02 -9.1382e-02 -1.0971e-01 -6.8371e-03\n",
       "      ⋮ \n",
       " \n",
       " (19,0 ,.,.) = \n",
       "   1.7455e-02  1.0734e-02  7.1749e-03  2.3432e-02 -1.3813e-02\n",
       "   1.5337e-02  8.9001e-02  1.4701e-01 -5.1441e-02 -1.6826e-01\n",
       "  -6.2666e-02  7.2406e-02  7.8767e-02 -1.4629e-01 -4.4736e-02\n",
       "   8.7483e-02 -6.9301e-02 -7.1658e-03  8.0913e-02 -1.0023e-01\n",
       "  -6.9090e-02  8.4156e-03  2.4275e-02 -1.5256e-02 -8.2432e-02\n",
       " \n",
       " (19,1 ,.,.) = \n",
       "   1.5592e-02  8.2650e-02  1.0585e-01  8.4347e-02  4.4096e-02\n",
       "  -1.8714e-02  7.4250e-02  1.0837e-01 -1.0913e-01 -4.1772e-02\n",
       "   9.3154e-02 -2.7006e-02  6.6352e-02  5.5310e-03  2.4246e-04\n",
       "   5.3427e-02  8.6745e-02  8.1236e-02 -6.5239e-02 -7.6923e-02\n",
       "   2.0937e-02 -1.2511e-01  3.2095e-02  7.9404e-02 -1.7148e-02\n",
       " \n",
       " (19,2 ,.,.) = \n",
       "   2.3578e-02  5.7887e-02  1.1134e-01  1.2142e-02  7.9302e-03\n",
       "  -1.2813e-02  7.2863e-02  7.8829e-02 -5.5224e-02  4.1885e-02\n",
       "   6.9201e-02 -6.4939e-02  2.5619e-02 -8.5729e-02  9.7503e-02\n",
       "   1.6647e-02 -2.1781e-03  2.9806e-02  7.3953e-02 -1.4523e-01\n",
       "  -6.1090e-02  4.7246e-02 -6.1431e-02 -4.5310e-02  1.7875e-02\n",
       "    ...\n",
       " \n",
       " (19,9 ,.,.) = \n",
       "   1.1727e-02  2.2604e-01 -1.2731e-02 -1.0117e-01 -1.3567e-01\n",
       "  -1.6669e-02 -1.2309e-01  6.0234e-02  1.3939e-02  2.1074e-02\n",
       "   8.9467e-02 -7.5616e-03  1.0649e-01  5.9097e-02 -3.5920e-02\n",
       "  -7.6159e-02 -2.5324e-02  1.2618e-01 -6.8371e-02  7.7665e-02\n",
       "  -9.5245e-02  8.6023e-02  1.0993e-01  1.0425e-01 -2.8844e-02\n",
       " \n",
       " (19,10,.,.) = \n",
       "  -7.6962e-02 -1.5427e-02  1.0691e-01 -6.4299e-02  8.7923e-02\n",
       "   1.1888e-01 -2.2695e-02  2.2735e-02 -2.9885e-02  8.6474e-02\n",
       "  -4.1152e-02 -3.1969e-02 -1.2190e-01  3.7693e-02  3.8335e-02\n",
       "  -3.6537e-02  6.4008e-02 -7.0874e-02  6.1036e-02 -6.0810e-02\n",
       "  -1.3562e-01 -3.9676e-02  3.6904e-03 -9.6733e-02  1.1909e-02\n",
       " \n",
       " (19,11,.,.) = \n",
       "  -1.1693e-01 -8.1097e-02 -1.5216e-01 -1.7488e-02 -9.2345e-02\n",
       "   4.6377e-02 -3.2217e-02  9.5913e-02 -6.8478e-02 -6.4504e-02\n",
       "  -1.0025e-01 -1.0175e-01 -4.5307e-03  5.2356e-02 -1.8086e-02\n",
       "   7.1452e-03 -6.9036e-02 -6.1330e-02  3.8875e-02  6.6673e-02\n",
       "  -4.7427e-02  4.7122e-03 -9.6220e-02  5.7547e-03 -2.6244e-02\n",
       " [torch.FloatTensor of size 20x12x5x5], Parameter containing:\n",
       "  0.1001\n",
       "  0.0997\n",
       "  0.1008\n",
       "  0.0998\n",
       "  0.1009\n",
       "  0.1002\n",
       "  0.1006\n",
       "  0.0997\n",
       "  0.1007\n",
       "  0.1001\n",
       "  0.1011\n",
       "  0.1006\n",
       "  0.1008\n",
       "  0.0997\n",
       "  0.1011\n",
       "  0.1004\n",
       "  0.0998\n",
       "  0.0997\n",
       "  0.1002\n",
       "  0.1006\n",
       " [torch.FloatTensor of size 20]]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.conv2.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_json('test.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
